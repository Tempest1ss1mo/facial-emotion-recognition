# Facial Emotion Recognition

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A CNN-based deep learning project for recognizing human emotions from facial images. The model classifies faces into seven emotion categories: **angry**, **disgust**, **fear**, **happy**, **sad**, **surprise**, and **neutral**.

## ğŸ¯ Objectives

1. Implement a baseline CNN for facial emotion classification
2. Apply transfer learning using pretrained architectures (ResNet18/VGG16)
3. Compare feature extraction vs. fine-tuning approaches
4. Visualize learned representations using Grad-CAM

## ğŸ“Š Dataset

This project uses the [FER-2013 dataset](https://www.kaggle.com/datasets/msambare/fer2013) from Kaggle:

| Split | Images |
|-------|--------|
| Train | 28,709 |
| Validation | 3,589 |
| Test | 3,589 |
| **Total** | **35,887** |

- Image size: 48Ã—48 grayscale
- 7 emotion categories

## ğŸ—ï¸ Project Structure

```
facial-emotion-recognition/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml           # Training configurations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baseline_cnn.py   # LeNet-inspired baseline model
â”‚   â”‚   â””â”€â”€ transfer_model.py # ResNet18/VGG16 transfer learning
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py        # Dataset loading and preprocessing
â”‚   â”‚   â””â”€â”€ augmentation.py   # Data augmentation transforms
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gradcam.py        # Grad-CAM visualization
â”‚   â”‚   â”œâ”€â”€ visualization.py  # Plotting utilities
â”‚   â”‚   â””â”€â”€ metrics.py        # Evaluation metrics
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â””â”€â”€ evaluate.py           # Evaluation script
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_baseline_training.ipynb
â”‚   â””â”€â”€ 03_transfer_learning.ipynb
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_models.py
â”œâ”€â”€ checkpoints/              # Saved model weights
â”œâ”€â”€ results/                  # Training logs and visualizations
â””â”€â”€ data/                     # Dataset directory (not tracked)
    â””â”€â”€ .gitkeep
```

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/Tempest1ss1mo/facial-emotion-recognition.git
cd facial-emotion-recognition
```

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Download Dataset

Download the FER-2013 dataset from [Kaggle](https://www.kaggle.com/datasets/msambare/fer2013) and place it in the `data/` directory:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ angry/
â”‚   â”œâ”€â”€ disgust/
â”‚   â”œâ”€â”€ fear/
â”‚   â”œâ”€â”€ happy/
â”‚   â”œâ”€â”€ sad/
â”‚   â”œâ”€â”€ surprise/
â”‚   â””â”€â”€ neutral/
â””â”€â”€ test/
    â”œâ”€â”€ angry/
    â”œâ”€â”€ ...
```

### 5. Train Models

**Train Baseline CNN:**
```bash
python src/train.py --model baseline --epochs 50
```

**Train Transfer Learning Model:**
```bash
python src/train.py --model resnet18 --mode finetune --epochs 30
```

### 6. Evaluate Model

```bash
python src/evaluate.py --checkpoint checkpoints/best_model.pth
```

## ğŸ§  Model Architecture

### Baseline CNN (LeNet-inspired)

```
Input (48Ã—48Ã—1)
    â†“
Conv2d(32) â†’ ReLU â†’ MaxPool
    â†“
Conv2d(64) â†’ ReLU â†’ MaxPool
    â†“
Conv2d(128) â†’ ReLU â†’ MaxPool
    â†“
Flatten â†’ Dropout(0.5)
    â†“
FC(512) â†’ ReLU â†’ Dropout(0.5)
    â†“
FC(7) â†’ Softmax
```

### Transfer Learning (ResNet18/VGG16)

- **Feature Extraction**: Freeze pretrained layers, train only classifier
- **Fine-tuning**: Unfreeze top layers for domain adaptation

## ğŸ“ˆ Expected Results

| Model | Expected Accuracy |
|-------|------------------|
| Baseline CNN | 60-65% |
| Transfer Learning (Fine-tuned) | 70-75% |

## ğŸ” Grad-CAM Visualization

Grad-CAM attention maps highlight which facial regions the model focuses on for predictions:

- Eyes region for surprise/fear
- Mouth region for happy/sad
- Overall facial structure for neutral

## âš™ï¸ Configuration

Training parameters can be modified in `configs/config.yaml`:

```yaml
model:
  name: resnet18
  pretrained: true
  num_classes: 7

training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.001
  optimizer: adam

augmentation:
  horizontal_flip: true
  rotation: 10
  zoom_range: 0.1
```

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 2.0+
- torchvision
- numpy
- pandas
- matplotlib
- seaborn
- opencv-python
- tqdm
- PyYAML

## ğŸ“ Training Tips

1. **Data Augmentation**: Essential for preventing overfitting
2. **Learning Rate Scheduler**: Use `ReduceLROnPlateau` for better convergence
3. **Class Imbalance**: Consider weighted loss for underrepresented emotions
4. **Early Stopping**: Monitor validation loss to prevent overfitting

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š References

1. [FER-2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013)
2. [Grad-CAM: Visual Explanations from Deep Networks](https://arxiv.org/abs/1610.02391)
3. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

## ğŸ‘¤ Author

**Mingliang Yu** (my2899)

---

â­ Star this repo if you find it helpful!
